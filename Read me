1. Imports and ConfigurationsThe script begins by importing essential libraries:PyTorch (torch): For building and training the neural network.Torchvision: For dataset access and image transformations.Matplotlib and NumPy: For plotting and numerical operations.Utilities: os for file handling, time for timing, and others for reproducibility.Key configurations and hyperparameters are defined:NUM_CLASSES = 10: Matches CIFAR-10’s 10 categories.BATCH_SIZE = 128: Number of samples per batch.NUM_EPOCHS = 50: Maximum training iterations.LEARNING_RATE = 0.001: Initial learning rate for optimization.DEVICE: Uses GPU (cuda) if available, otherwise CPU.Directories for TensorBoard logs (./runs) and model checkpoints (./models).To ensure reproducibility, seeds are set for torch, numpy, and random.2. TensorBoard InitializationA SummaryWriter is initialized to log training metrics (e.g., loss, accuracy) to TensorBoard, enabling real-time visualization of the training process.3. Data Preparation and AugmentationThe CIFAR-10 dataset is downloaded and preprocessed with transformations:Training Transforms:RandomCrop(32, padding=4): Adds variety by cropping with padding.RandomHorizontalFlip(): Randomly flips images horizontally.ToTensor(): Converts images to PyTorch tensors.Normalize(): Applies mean (0.4914, 0.4822, 0.4465) and std (0.2023, 0.1994, 0.2010) specific to CIFAR-10.Test Transforms: Only tensor conversion and normalization, avoiding augmentation.The training data is split:90% (45,000 images) for training.10% (5,000 images) for validation, created using Subset and shuffled indices.DataLoaders are set up for training, validation, and testing with BATCH_SIZE=128 and num_workers=2 for parallel loading.Class names are defined for interpretability: airplane, automobile, etc.4. Model Definition: Enhanced CNNThe EnhancedCNN class defines a custom convolutional neural network:Architecture:Conv Block 1: 3 input channels → 64 channels, two Conv2d layers (3x3 kernels), BatchNorm, ReLU, MaxPool (2x2), Dropout (0.25).Conv Block 2: 64 → 128 channels, similar structure.Conv Block 3: 128 → 256 channels, similar structure.Fully Connected Layers: Flattens to 256×4×4 (after three poolings), then Linear (4096 → 512), ReLU, Dropout (0.5), Linear (512 → 10).Features: Batch normalization for stability, dropout for regularization, and ReLU for non-linearity.The model is instantiated and moved to the DEVICE.5. Loss, Optimizer, and SchedulerLoss: CrossEntropyLoss, ideal for multi-class classification.Optimizer: Adam with a learning rate of 0.001.Scheduler: ReduceLROnPlateau, reduces the learning rate by a factor of 0.5 if validation loss plateaus for 5 epochs.6. Utility FunctionsAccuracy Calculation: Compares predicted classes (highest output scores) to true labels.Early Stopping: Stops training if validation loss doesn’t improve for 10 epochs, saving the best model weights.7. Training LoopThe train_model function implements training and validation:Training Phase:Model in train mode.Forward pass, loss computation, backpropagation, and optimization.Logs loss and accuracy every 50 batches and per epoch to TensorBoard.Validation Phase:Model in eval mode, no gradients.Computes validation loss and accuracy.Features:Saves checkpoints per epoch (e.g., checkpoint_epoch_1.pth).Adjusts learning rate via the scheduler.Applies early stopping.Tracks the best model based on validation accuracy.Returns the trained model and loss histories.8. Visualization of Training ProgressThe plot_metrics function plots training and validation loss curves, saved as loss_curve.png, to diagnose overfitting or underfitting.9. Evaluation on Test SetThe evaluate_model function assesses the model on the test set:Computes average loss and accuracy.Collects all predictions and true labels for further analysis.Outputs: e.g., “Test Set - Loss: 0.XXXX Accuracy: 0.XXXX”.10. Model SavingThe final trained model is saved as final_enhanced_cnn.pth.11. Prediction VisualizationA batch of test images is displayed with ground truth and predicted labels, using imshow to unnormalize and plot images via Matplotlib.12. Learning Rate ExplorationThe explore_learning_rates function tests 10 learning rates (from 10⁻⁵ to 10⁻²) on 10 batches, plotting loss vs. learning rate to guide hyperparameter tuning.13. Custom Dataset WrapperCustomCIFAR10Dataset extends CIFAR-10 functionality (partially implemented), potentially for adding metadata or debugging.
